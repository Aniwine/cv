{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 官方评测算子\n",
    "在配置文件中或直接给Runner传入val_evaluator或test_evaluator参数，单个评测指标传入字典，多个评测指标传入字典组成的列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_evaluator = dict(type='Accuracy', top_k=(1, 5))  # 使用分类正确率评测指标\n",
    "test_evaluator = [\n",
    "    # 目标检测指标\n",
    "    dict(\n",
    "        type='CocoMetric',\n",
    "        metric=['bbox', 'segm'],\n",
    "        ann_file='annotations/instances_val2017.json',\n",
    "    ),\n",
    "    # 全景分割指标\n",
    "    dict(\n",
    "        type='CocoPanopticMetric',\n",
    "        ann_file='annotations/panoptic_val2017.json',\n",
    "        seg_prefix='annotations/panoptic_val2017',\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 自定义评测指标\n",
    "1. 继承BaseMetrics，实现process\n",
    "2. 实现compute_metrics\n",
    "3. 可选default_prefix参数\n",
    "\n",
    "default_prefix参数为评价指标的前缀，是评测算子的类属性，也可在配置文件中通过prefix参数指定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.registry import METRICS\n",
    "from mmengine.evaluator import BaseMetric\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "\n",
    "@METRICS.register_module()\n",
    "class SimpleAccuracy(BaseMetric):\n",
    "    default_prefix = 'Acc'\n",
    "    def process(self,data_batch:Sequence[dict],data_samples:Sequence[dict]):\n",
    "        #data_batch是来自于dataloader的数据，data_samples是来自于模型的输出\n",
    "        result={\n",
    "            'pred':data_samples['pred_label'],#bs*1\n",
    "            'gt':data_samples['data_sample']['gt_label']\n",
    "        }\n",
    "        self.results.append(result)\n",
    "\n",
    "    def compute_metrics(self,results):\n",
    "        preds=np.concatenate([result['pred'] for result in results])\n",
    "        gts=np.concatenate([result['gt'] for result in results])\n",
    "\n",
    "        acc=(preds==gts).sum()/len(gts)\n",
    "        return {'accuracy':acc}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
