{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 单精度训练示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch版本\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import SGD\n",
    "\n",
    "inputs=[torch.zeros(10,1,1)]*10\n",
    "targets=[torch.ones(10,1,1)]*10\n",
    "model=nn.Linear(1,1)\n",
    "optimizer=SGD(model.parameters(),lr=0.01)   \n",
    "#梯度清零\n",
    "optimizer.zero_grad()\n",
    "#训练\n",
    "for input,target in zip(inputs,targets):\n",
    "    pred=model(input)\n",
    "    loss=F.l1_loss(pred,target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mmengine版本\n",
    "from mmengine.optim import OptimWrapper\n",
    "\n",
    "inputs=[torch.zeros(10,1,1)]*10\n",
    "targets=[torch.ones(10,1,1)]*10\n",
    "model=nn.Linear(1,1)\n",
    "optimizer=SGD(model.parameters(),lr=0.01) \n",
    "#构造优化器包装器\n",
    "optimizer_wrapper=OptimWrapper(optimizer=optimizer)\n",
    "\n",
    "for input,target in zip(inputs,targets):\n",
    "    pred=model(input)\n",
    "    loss=F.l1_loss(pred,target)\n",
    "    optimizer_wrapper.update_params(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 混合精度训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch版本\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "inputs=[torch.zeros(10,1,1)]*10\n",
    "targets=[torch.ones(10,1,1)]*10\n",
    "model=nn.Linear(1,1).cuda()\n",
    "optimizer=SGD(model.parameters(),lr=0.01)\n",
    "#梯度清零\n",
    "optimizer.zero_grad()\n",
    "\n",
    "for input,target in zip(inputs,targets):\n",
    "    with autocast():\n",
    "        pred=model(input.cuda())\n",
    "    loss=F.l1_loss(pred,target.cuda())\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mmengine版本\n",
    "from mmengine.optim import AmpOptimWrapper\n",
    "\n",
    "inputs=[torch.zeros(10,1,1)]*10\n",
    "targets=[torch.ones(10,1,1)]*10\n",
    "model=nn.Linear(1,1).cuda()\n",
    "optimizer=SGD(model.parameters(),lr=0.01)\n",
    "#构造优化器包装器\n",
    "optimizer_wrapper=AmpOptimWrapper(optimizer=optimizer)\n",
    "for input,target in zip(inputs,targets):\n",
    "    with optimizer_wrapper.optim_context(model):\n",
    "        pred=model(input.cuda())\n",
    "    loss=F.l1_loss(pred,target.cuda())\n",
    "    optimizer_wrapper.update_params(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 混合精度和梯度累加"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch版本\n",
    "#梯度累加实现原理即为只在某些步骤进行梯度更新和梯度清零\n",
    "for idx,(input,target) in enumerate(zip(inputs,targets)):\n",
    "    with autocast():\n",
    "        pred=model(input.cuda())\n",
    "    loss=F.l1_loss(pred,target.cuda())\n",
    "    loss.backward()\n",
    "    if idx%2==0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mmengine版本\n",
    "optimizer_wrapper =AmpOptimWrapper(optimizer=optimizer,accumulative_counts=2)\n",
    "\n",
    "for idx,(input,target) in enumerate(zip(inputs,targets)):\n",
    "    with optimizer_wrapper.optim_context(model):\n",
    "        pred=model(input.cuda())\n",
    "    loss=F.l1_loss(pred,target.cuda())\n",
    "    optimizer_wrapper.update_params(loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mmengine的优化器封装同样实现了backward，step，zero_grad等接口，用户可以像普通优化器一样使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用mmengine实现pytorch风格的优化器\n",
    "for idx,(input,target) in enumerate(zip(inputs,targets)):\n",
    "    #梯度清零\n",
    "    optimizer_wrapper.zero_grad()\n",
    "    with optimizer_wrapper.optim_context(model):\n",
    "        pred=model(input.cuda())\n",
    "    loss=F.l1_loss(pred,target.cuda())\n",
    "    #显著的区别就在此处，不再是loss.backward()\n",
    "    optimizer_wrapper.backward(loss)\n",
    "    if idx%2==0:\n",
    "        optimizer_wrapper.step()\n",
    "        optimizer_wrapper.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "梯度裁剪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_wrapper=AmpOptimWrapper(optimizer=optimizer,clip_grad=dict(max_norm=1.))\n",
    "optimizer_wrapper=AmpOptimWrapper(optimizer=optimizer,clip_grad=dict(clip_value=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "获取学习率和动量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/mmyolo/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': [0.01]}\n",
      "{'momentum': [0.9]}\n",
      "0.01\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "from mmengine.optim import OptimWrapper\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "\n",
    "model=nn.Linear(10,10)\n",
    "optimizer=SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "optimizer_wrapper=OptimWrapper(optimizer=optimizer)\n",
    "print(optimizer_wrapper.get_lr())\n",
    "print(optimizer_wrapper.get_momentum())\n",
    "print(optimizer.param_groups[0]['lr'])\n",
    "print(optimizer.param_groups[0]['momentum'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导出/加载优化器状态字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'params': [0, 1]}]}\n",
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'params': [0, 1]}], 'loss_scaler': {'scale': 65536.0, 'growth_factor': 2.0, 'backoff_factor': 0.5, 'growth_interval': 2000, '_growth_tracker': 0}}\n",
      "********************\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "from torch.optim import SGD\n",
    "from mmengine.optim import OptimWrapper,AmpOptimWrapper\n",
    "\n",
    "model=nn.Linear(10,10)\n",
    "optimizer=SGD(model.parameters(),lr=0.01,momentum=0.9)\n",
    "\n",
    "optimizer_wrapper=OptimWrapper(optimizer=optimizer)\n",
    "amp_optimizer_wrapper=AmpOptimWrapper(optimizer=optimizer)\n",
    "\n",
    "#导出状态字典\n",
    "optim_state_dict=optimizer_wrapper.state_dict()\n",
    "amp_state_dict=amp_optimizer_wrapper.state_dict()\n",
    "print(optim_state_dict)\n",
    "print(amp_state_dict)\n",
    "print('*'*20)\n",
    "#加载状态字典\n",
    "optim_wrapper_new=OptimWrapper(optimizer=optimizer)\n",
    "amp_optimizer_wrapper_new=AmpOptimWrapper(optimizer=optimizer)\n",
    "optim_wrapper_new.load_state_dict(optim_state_dict)\n",
    "amp_optimizer_wrapper_new.load_state_dict(amp_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器字典：优化器字典并没有实现update_params等方法，不能直接用于训练中更新参数等，但有get_lr等方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gen.lr': [0.01], 'disc.lr': [0.01]}\n",
      "{'gen.momentum': [0.9], 'disc.momentum': [0.9]}\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "import torch.nn as nn\n",
    "from mmengine.optim import OptimWrapper,OptimWrapperDict\n",
    "\n",
    "gen=nn.Linear(10,10)\n",
    "disc=nn.Linear(10,10)\n",
    "\n",
    "optim_gen=SGD(gen.parameters(),lr=0.01,momentum=0.9)\n",
    "disc_gen=SGD(disc.parameters(),lr=0.01,momentum=0.9)\n",
    "\n",
    "optim_wrapper_gen=OptimWrapper(optimizer=optim_gen)\n",
    "optim_wrapper_disc=OptimWrapper(optimizer=disc_gen)\n",
    "optim_wrapper_dict=OptimWrapperDict(gen=optim_wrapper_gen,disc=optim_wrapper_disc)\n",
    "print(optim_wrapper_dict.get_lr())\n",
    "print(optim_wrapper_dict.get_momentum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "执行器中配置优化器封装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=dict(type='SGD',lr=0.01,momentum=0.9)\n",
    "optim_wrapper=dict(type='OptimWrapper',optimizer=optimizer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
