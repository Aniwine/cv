{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集基类的标准化流程：\n",
    "load metainfo-> join path(应该是data_root和image_prefix、ann_file的拼接)->build pipeline->\n",
    "load data_list->..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.dataset import BaseDataset\n",
    "import os.path as osp\n",
    "\n",
    "class ToyDataset(BaseDataset):\n",
    "    def parse_data_info(self,raw_data_info):\n",
    "        #raw_data_info代表list[dict]中的一个字典\n",
    "        data_info=raw_data_info\n",
    "        image_prefix=self.data_prefix.get('img_path',None)\n",
    "        if image_prefix is not None:\n",
    "            data_info['img_path']=osp.join(image_prefix,data_info['img_path'])\n",
    "        return data_info\n",
    "#注意img_path字段，既是图像前缀的字段名，也是图像文件的字段名（不如mmcv的filename）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/annotations/train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 18\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m     13\u001b[0m pipeline \u001b[39m=\u001b[39m [\n\u001b[1;32m     14\u001b[0m     LoadImage(),\n\u001b[1;32m     15\u001b[0m     ParseImage(),\n\u001b[1;32m     16\u001b[0m ]\n\u001b[0;32m---> 18\u001b[0m toy_dataset \u001b[39m=\u001b[39m ToyDataset(\n\u001b[1;32m     19\u001b[0m     data_root\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mdata/\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m     data_prefix\u001b[39m=\u001b[39;49m\u001b[39mdict\u001b[39;49m(img_path\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain/\u001b[39;49m\u001b[39m'\u001b[39;49m),\n\u001b[1;32m     21\u001b[0m     ann_file\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mannotations/train.json\u001b[39;49m\u001b[39m'\u001b[39;49m,\n\u001b[1;32m     22\u001b[0m     pipeline\u001b[39m=\u001b[39;49mpipeline)\n",
      "File \u001b[0;32m~/anaconda3/envs/mmyolo/lib/python3.10/site-packages/mmengine/dataset/base_dataset.py:245\u001b[0m, in \u001b[0;36mBaseDataset.__init__\u001b[0;34m(self, ann_file, metainfo, data_root, data_prefix, filter_cfg, indices, serialize_data, pipeline, test_mode, lazy_init, max_refetch)\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[39m# Full initialize the dataset.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m lazy_init:\n\u001b[0;32m--> 245\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfull_init()\n",
      "File \u001b[0;32m~/anaconda3/envs/mmyolo/lib/python3.10/site-packages/mmengine/dataset/base_dataset.py:296\u001b[0m, in \u001b[0;36mBaseDataset.full_init\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[39m# load data information\u001b[39;00m\n\u001b[0;32m--> 296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_data_list()\n\u001b[1;32m    297\u001b[0m \u001b[39m# filter illegal data, such as data that has no annotations.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_list \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_data()\n",
      "File \u001b[0;32m~/anaconda3/envs/mmyolo/lib/python3.10/site-packages/mmengine/dataset/base_dataset.py:433\u001b[0m, in \u001b[0;36mBaseDataset.load_data_list\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Load annotations from an annotation file named as ``self.ann_file``\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \n\u001b[1;32m    422\u001b[0m \u001b[39mIf the annotation file does not follow `OpenMMLab 2.0 format dataset\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m    list[dict]: A list of annotation.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[39m\"\"\"\u001b[39;00m  \u001b[39m# noqa: E501\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[39m# `self.ann_file` denotes the absolute annotation file path if\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m# `self.root=None` or relative path if `self.root=/path/to/data/`.\u001b[39;00m\n\u001b[0;32m--> 433\u001b[0m annotations \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mann_file)\n\u001b[1;32m    434\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(annotations, \u001b[39mdict\u001b[39m):\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mThe annotations loaded from annotation file \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    436\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mshould be a dict, but got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(annotations)\u001b[39m}\u001b[39;00m\u001b[39m!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/mmyolo/lib/python3.10/site-packages/mmengine/fileio/io.py:852\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, file_format, file_client_args, backend_args, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m     file_backend \u001b[39m=\u001b[39m get_file_backend(file, backend_args\u001b[39m=\u001b[39mbackend_args)\n\u001b[1;32m    851\u001b[0m \u001b[39mif\u001b[39;00m handler\u001b[39m.\u001b[39mstr_like:\n\u001b[0;32m--> 852\u001b[0m     \u001b[39mwith\u001b[39;00m StringIO(file_backend\u001b[39m.\u001b[39;49mget_text(file)) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m    853\u001b[0m         obj \u001b[39m=\u001b[39m handler\u001b[39m.\u001b[39mload_from_fileobj(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    854\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/mmyolo/lib/python3.10/site-packages/mmengine/fileio/backends/local_backend.py:56\u001b[0m, in \u001b[0;36mLocalBackend.get_text\u001b[0;34m(self, filepath, encoding)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_text\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m     38\u001b[0m              filepath: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     39\u001b[0m              encoding: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     40\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Read text from a given ``filepath`` with 'r' mode.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \n\u001b[1;32m     42\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[39m        'hello world'\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filepath, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[1;32m     57\u001b[0m         text \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[1;32m     58\u001b[0m     \u001b[39mreturn\u001b[39;00m text\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/annotations/train.json'"
     ]
    }
   ],
   "source": [
    "\n",
    "class LoadImage:\n",
    "\n",
    "    def __call__(self, results):\n",
    "        results['img'] = cv2.imread(results['img_path'])\n",
    "        return results\n",
    "\n",
    "class ParseImage:\n",
    "\n",
    "    def __call__(self, results):\n",
    "        results['img_shape'] = results['img'].shape\n",
    "        return results\n",
    "\n",
    "pipeline = [\n",
    "    LoadImage(),\n",
    "    ParseImage(),\n",
    "]\n",
    "\n",
    "toy_dataset = ToyDataset(\n",
    "    data_root='data/',\n",
    "    data_prefix=dict(img_path='train/'),\n",
    "    ann_file='annotations/train.json',\n",
    "    pipeline=pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一个原始数据可能只包含一个训练样本，也可能包含多个训练样本，比如视频帧"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parse_data_info方法解析数据，他的入参raw_data_info是调用了load_data_info函数后的返回值，是\n",
    "list[dict],所以对于不满足规范的数据集，重载该方法即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "懒加载，配置文件的lazy_init=True，只会执行前三个步骤，即load meatinfo,join path,build pipeline\n",
    "后续要是用数据集的信息，需要手动执行dataset.full_init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据集基类包装器\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#连接多个数据集\n",
    "from mmengine.dataset import ConcatDataset\n",
    "\n",
    "dataset1,dataset2=ToyDataset(),ToyDataset()\n",
    "my_dataset=ConcatDataset(datasets=[dataset1,dataset2])\n",
    "\n",
    "#不难推导配置文件写法为\n",
    "my_dataset=dict(\n",
    "    type='ConcatDataset',\n",
    "    datasets=[dict(type='ToyDataset'),dict(type='ToyDataset')]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重复采样数据集\n",
    "from mmengine.dataset import RepeatDataset\n",
    "my_dataset=RepeatDataset(ToyDataset(),times=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#类别平衡数据集\n",
    "#注意：该数据集包装器需要被包装的数据集支持get_cat_ids方法，也就是根据某idx获取其训练样本的类别\n",
    "from mmengine.dataset import ClassBalancedDataset,BaseDataset\n",
    "\n",
    "class ToyDataset(BaseDataset):\n",
    "    def parse_data_info(self,raw_data_info):\n",
    "        #raw_data_info代表list[dict]中的一个字典\n",
    "        data_info=raw_data_info\n",
    "        image_prefix=self.data_prefix.get('img_path',None)\n",
    "        if image_prefix is not None:\n",
    "            data_info['img_path']=osp.join(image_prefix,data_info['img_path'])\n",
    "        return data_info\n",
    "    \n",
    "    def get_cat_ids(self,idx):\n",
    "        data_info=self.get_data_info(idx)\n",
    "        return [int(data_info['img_label'])]\n",
    "my_dataset=ClassBalancedDataset(dataset=ToyDataset(),oversample_thr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmengine.dataset import BaseDataset\n",
    "from mmengine.registry import DATASETS\n",
    "import copy\n",
    "\n",
    "#如果需要自定义数据集包装器，需要实现以下的一些部分\n",
    "DATASETS.register_module()\n",
    "class ExampleDatasetWrapper():\n",
    "\n",
    "    def __init__(self,dataset,lazy_init=False):\n",
    "        #构建原始数据集\n",
    "        if isinstance(dataset,dict):\n",
    "            self.dataset=DATASETS.build(dataset)\n",
    "        elif isinstance(dataset,BaseDataset):\n",
    "            self.dataset=dataset\n",
    "        else:\n",
    "            raise TypeError('dataset must be a dict or BaseDataset')\n",
    "        \n",
    "        self._metainfo=self.dataset.metainfo\n",
    "        \"\"\"\n",
    "        1.包装器的超参数\n",
    "        \"\"\"\n",
    "\n",
    "        self._full_initalized=False\n",
    "\n",
    "        if not lazy_init:\n",
    "            self.init_full()\n",
    "    \n",
    "    def full_init(self):\n",
    "        if self._full_initalized:\n",
    "            return\n",
    "        \n",
    "        self.full_init()\n",
    "        \"\"\"\"2.实现包装数据集\"\"\"\n",
    "        self._full_initalized=True\n",
    "\n",
    "    @force_full_init\n",
    "    def _get_ori_dataset_idx(self,idx):\n",
    "        \"\"\"3.实现将索引idx映射到原始数据集的索引ori_idx\"\"\"\n",
    "        ori_idx=2*idx\n",
    "        return ori_idx\n",
    "    #以下是提供与原始dataset一致的对外接口\n",
    "\n",
    "    @force_full_init\n",
    "    def get_data_info(self,idx):\n",
    "        sample_idx=self._get_ori_dataset_idx(idx)\n",
    "        return self.dataset.get_data_info(sample_idx)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if not self._fully_initialized:\n",
    "            warnings.warn('Please call `full_init` method manually to '\n",
    "                          'accelerate the speed.')\n",
    "            self.full_init()\n",
    "\n",
    "        sample_idx = self._get_ori_dataset_idx(idx)\n",
    "        return self.dataset[sample_idx]\n",
    "    \n",
    "    @force_full_init\n",
    "    def __len__(self):\n",
    "        \"\"\"4.实现获取包装数据集的数据长度\"\"\"\n",
    "        len_wrapper=len(self.dataset)\n",
    "        return len_wrapper\n",
    "    \n",
    "    @property\n",
    "    def metainfo(self):\n",
    "        return copy.deepcopy(self._metainfo)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mmyolo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
